{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ACT-UNet: Adaptive Computation Time for U-Net Segmentation\n",
        "\n",
        "This notebook demonstrates the ACT-enhanced U-Net with reinforcement learning-based adaptive bottleneck depth.\n",
        "\n",
        "## Overview\n",
        "- **Model**: U-Net with ACT bottleneck that learns when to stop iterative refinement\n",
        "- **Dataset**: Oxford-IIIT Pets (binary segmentation)\n",
        "- **Training**: Actor-Critic RL approach for learning halting policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('datasets', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Download\n",
        "Download and prepare the Oxford-IIIT Pets dataset for segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data download\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "print(\"Downloading Oxford-IIIT Pets dataset...\")\n",
        "print(\"This will download ~800MB of data on first run\\n\")\n",
        "\n",
        "# Download dataset with segmentation masks\n",
        "dataset = OxfordIIITPet(\n",
        "    root='./datasets',\n",
        "    split='trainval', \n",
        "    target_types='segmentation',\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "# Also download test split\n",
        "test_dataset = OxfordIIITPet(\n",
        "    root='./datasets',\n",
        "    split='test',\n",
        "    target_types='segmentation', \n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "print(f\"✓ Training samples: {len(dataset)}\")\n",
        "print(f\"✓ Test samples: {len(test_dataset)}\")\n",
        "print(\"\\nDataset downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset and Create DataLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "from data.pets import PetsDataset\n",
        "\n",
        "# Configuration\n",
        "batch_size = 8\n",
        "image_size = (256, 256)\n",
        "num_workers = 2\n",
        "\n",
        "# Get data loaders\n",
        "train_loader, test_loader = PetsDataset.get_data_loaders(\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    root='./datasets',\n",
        "    size=image_size,\n",
        "    download=False  # Already downloaded\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Image size: {image_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize samples\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_samples(loader, num_samples=4):\n",
        "    batch = next(iter(loader))\n",
        "    images = batch['image'][:num_samples]\n",
        "    masks = batch['mask'][:num_samples]\n",
        "    \n",
        "    # Denormalize images for visualization\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "    images = images * std + mean\n",
        "    images = torch.clamp(images, 0, 1)\n",
        "    \n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, num_samples*3))\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        # Original image\n",
        "        axes[i, 0].imshow(images[i].permute(1, 2, 0))\n",
        "        axes[i, 0].set_title('Input Image')\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Ground truth mask\n",
        "        axes[i, 1].imshow(masks[i, 0], cmap='gray')\n",
        "        axes[i, 1].set_title('Ground Truth Mask')\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Overlay\n",
        "        axes[i, 2].imshow(images[i].permute(1, 2, 0))\n",
        "        axes[i, 2].imshow(masks[i, 0], alpha=0.5, cmap='jet')\n",
        "        axes[i, 2].set_title('Overlay')\n",
        "        axes[i, 2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_samples(train_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Models\n",
        "We'll create both standard U-Net and ACT-enhanced U-Net for comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models\n",
        "from model.unet_base import UNet\n",
        "from model.act_unet import ACTUNet\n",
        "\n",
        "# Standard U-Net (baseline)\n",
        "unet_baseline = UNet(n_channels=3, n_classes=1, bilinear=True).to(device)\n",
        "\n",
        "# ACT-enhanced U-Net\n",
        "act_unet = ACTUNet(\n",
        "    n_channels=3, \n",
        "    n_classes=1,\n",
        "    max_iterations=5,  # Maximum K iterations in bottleneck\n",
        "    bilinear=True\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Standard U-Net parameters: {count_parameters(unet_baseline):,}\")\n",
        "print(f\"ACT-UNet parameters: {count_parameters(act_unet):,}\")\n",
        "print(f\"Max iterations (K): {act_unet.max_iterations}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training ACT-UNet\n",
        "Train the model with RL-based actor-critic updates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "from training.train import ACTTrainer\n",
        "\n",
        "trainer = ACTTrainer(\n",
        "    model=act_unet,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=test_loader,\n",
        "    device=device,\n",
        "    checkpoint_dir='./checkpoints'\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized!\")\n",
        "print(f\"Starting from epoch: {trainer.epoch}\")\n",
        "print(f\"Actor-Critic alternating frequency: {trainer.alternating_freq} steps\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model (set small number for demo, increase for full training)\n",
        "num_epochs = 2  # Increase to 50-100 for full training\n",
        "\n",
        "train_metrics, val_metrics = trainer.train(\n",
        "    num_epochs=num_epochs,\n",
        "    save_freq=5,\n",
        "    val_freq=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model and visualize results\n",
        "@torch.no_grad()\n",
        "def evaluate_and_visualize(model, loader, num_samples=4):\n",
        "    model.eval()\n",
        "    batch = next(iter(loader))\n",
        "    images = batch['image'][:num_samples].to(device)\n",
        "    masks = batch['mask'][:num_samples].to(device)\n",
        "    \n",
        "    # Get predictions with ACT info\n",
        "    if hasattr(model, 'bottleneck'):  # ACT-UNet\n",
        "        predictions, act_info = model(images, return_act_info=True)\n",
        "        halt_iters = act_info['halt_iterations'].cpu().numpy()\n",
        "    else:  # Standard U-Net\n",
        "        predictions = model(images)\n",
        "        halt_iters = None\n",
        "    \n",
        "    # Convert to probabilities\n",
        "    pred_probs = torch.sigmoid(predictions).cpu()\n",
        "    pred_binary = (pred_probs > 0.5).float()\n",
        "    \n",
        "    # Denormalize images\n",
        "    images_cpu = images.cpu()\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "    images_vis = images_cpu * std + mean\n",
        "    images_vis = torch.clamp(images_vis, 0, 1)\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(num_samples, 5, figsize=(15, num_samples*3))\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        # Input image\n",
        "        axes[i, 0].imshow(images_vis[i].permute(1, 2, 0))\n",
        "        axes[i, 0].set_title('Input')\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Ground truth\n",
        "        axes[i, 1].imshow(masks[i, 0].cpu(), cmap='gray')\n",
        "        axes[i, 1].set_title('Ground Truth')\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Prediction\n",
        "        axes[i, 2].imshow(pred_binary[i, 0], cmap='gray')\n",
        "        if halt_iters is not None:\n",
        "            axes[i, 2].set_title(f'Prediction (k={halt_iters[i]})')\n",
        "        else:\n",
        "            axes[i, 2].set_title('Prediction')\n",
        "        axes[i, 2].axis('off')\n",
        "        \n",
        "        # Probability map\n",
        "        axes[i, 3].imshow(pred_probs[i, 0], cmap='viridis', vmin=0, vmax=1)\n",
        "        axes[i, 3].set_title('Probability')\n",
        "        axes[i, 3].axis('off')\n",
        "        \n",
        "        # Error map\n",
        "        error = torch.abs(pred_binary[i, 0] - masks[i, 0].cpu())\n",
        "        axes[i, 4].imshow(error, cmap='hot')\n",
        "        axes[i, 4].set_title('Error')\n",
        "        axes[i, 4].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print ACT statistics if available\n",
        "    if halt_iters is not None:\n",
        "        print(f\"Halt iterations: {halt_iters}\")\n",
        "        print(f\"Average iterations: {halt_iters.mean():.2f}\")\n",
        "        print(f\"Computation saved: {(1 - halt_iters.mean()/model.max_iterations)*100:.1f}%\")\n",
        "\n",
        "# Evaluate ACT-UNet\n",
        "print(\"ACT-UNet Results:\")\n",
        "evaluate_and_visualize(act_unet, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptive Depth Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze adaptive depth distribution\n",
        "@torch.no_grad()\n",
        "def analyze_depth_distribution(model, loader, num_batches=10):\n",
        "    model.eval()\n",
        "    all_iterations = []\n",
        "    all_difficulties = []  # We'll use loss as a proxy for difficulty\n",
        "    \n",
        "    for i, batch in enumerate(loader):\n",
        "        if i >= num_batches:\n",
        "            break\n",
        "            \n",
        "        images = batch['image'].to(device)\n",
        "        masks = batch['mask'].to(device)\n",
        "        \n",
        "        # Get predictions with ACT info\n",
        "        predictions, act_info = model(images, return_act_info=True)\n",
        "        halt_iters = act_info['halt_iterations'].cpu().numpy()\n",
        "        all_iterations.extend(halt_iters)\n",
        "        \n",
        "        # Compute per-sample loss as difficulty proxy\n",
        "        from training.losses import SegmentationLoss\n",
        "        loss_fn = SegmentationLoss()\n",
        "        for j in range(images.shape[0]):\n",
        "            loss = loss_fn(predictions[j:j+1], masks[j:j+1]).item()\n",
        "            all_difficulties.append(loss)\n",
        "    \n",
        "    all_iterations = np.array(all_iterations)\n",
        "    all_difficulties = np.array(all_difficulties)\n",
        "    \n",
        "    # Plot distribution\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    # Histogram of iterations\n",
        "    axes[0].hist(all_iterations, bins=model.max_iterations, edgecolor='black')\n",
        "    axes[0].set_xlabel('Number of Iterations (k)')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('Distribution of Adaptive Depths')\n",
        "    axes[0].axvline(all_iterations.mean(), color='red', linestyle='--', \n",
        "                    label=f'Mean: {all_iterations.mean():.2f}')\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # Difficulty vs Iterations\n",
        "    axes[1].scatter(all_difficulties, all_iterations, alpha=0.5)\n",
        "    axes[1].set_xlabel('Task Difficulty (Loss)')\n",
        "    axes[1].set_ylabel('Iterations Used')\n",
        "    axes[1].set_title('Adaptive Depth vs Difficulty')\n",
        "    \n",
        "    # Iteration counts\n",
        "    unique, counts = np.unique(all_iterations, return_counts=True)\n",
        "    axes[2].bar(unique, counts, edgecolor='black')\n",
        "    axes[2].set_xlabel('Iterations')\n",
        "    axes[2].set_ylabel('Count')\n",
        "    axes[2].set_title('Iteration Usage')\n",
        "    axes[2].set_xticks(range(model.max_iterations))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\\\nAdaptive Depth Statistics:\")\n",
        "    print(f\"Mean iterations: {all_iterations.mean():.2f}\")\n",
        "    print(f\"Std iterations: {all_iterations.std():.2f}\")\n",
        "    print(f\"Min iterations: {all_iterations.min()}\")\n",
        "    print(f\"Max iterations: {all_iterations.max()}\")\n",
        "    print(f\"Computation saved: {(1 - all_iterations.mean()/model.max_iterations)*100:.1f}%\")\n",
        "    \n",
        "    # Correlation with difficulty\n",
        "    correlation = np.corrcoef(all_difficulties, all_iterations)[0, 1]\n",
        "    print(f\"Correlation (difficulty vs iterations): {correlation:.3f}\")\n",
        "\n",
        "# Analyze depth distribution\n",
        "if hasattr(act_unet, 'bottleneck'):\n",
        "    analyze_depth_distribution(act_unet, test_loader)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
